<!DOCTYPE html>

<html lang="fr">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Interface Gradio + Ollama - ComfyUI MCP</title>
<style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        .badge {
            display: inline-block;
            padding: 5px 15px;
            background: rgba(255,255,255,0.2);
            border-radius: 20px;
            font-size: 0.9em;
            margin: 5px;
        }
        .card {
            background: white;
            padding: 30px;
            margin: 20px 0;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .info-box {
            background: #e7f3ff;
            border-left: 5px solid #2196F3;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .success-box {
            background: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        h2 { 
            color: #667eea; 
            margin: 30px 0 15px 0;
            font-size: 1.8em;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
        }
        h3 { 
            color: #764ba2; 
            margin: 20px 0 10px 0;
        }
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .step {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 10px 20px;
            border-radius: 25px;
            display: inline-block;
            margin: 20px 0 10px 0;
            font-weight: bold;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background: #667eea;
            color: white;
        }
        tr:hover {
            background: #f5f5f5;
        }
        ul {
            margin-left: 20px;
            margin-top: 10px;
        }
        li {
            margin: 10px 0;
        }
        .icon { font-size: 1.5em; margin-right: 10px; }
        .btn {
            display: inline-block;
            padding: 12px 24px;
            background: #667eea;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            margin: 10px 5px;
            transition: background 0.3s;
        }
        .btn:hover {
            background: #5568d3;
        }
    </style>
</head>
<body>
<div class="container">
<header>
<h1>ü¶ô Interface Gradio + Ollama</h1>
<p>Chatbot 100% Local avec Outils ComfyUI</p>
<div>
<span class="badge">üü¢ 100% Gratuit</span>
<span class="badge">üîí 100% Local</span>
<span class="badge">‚ö° Rapide</span>
<span class="badge">üéØ D√©butant</span>
</div>
</header>
<div class="info-box">
<h3><span class="icon">‚ÑπÔ∏è</span>Qu'est-ce que cette m√©thode ?</h3>
<p>Cette interface vous permet de discuter avec un mod√®le IA <strong>enti√®rement local</strong> (Ollama) qui peut utiliser les outils de votre serveur MCP ComfyUI. Tout fonctionne sur votre machine, aucune connexion Internet requise apr√®s l'installation.</p>
<p><strong>Avantages</strong> : Gratuit, priv√©, rapide, fonctionne hors ligne</p>
<p><strong>Inconv√©nients</strong> : N√©cessite un GPU/CPU puissant pour les gros mod√®les</p>
</div>
<div class="card">
<h2>üìã Pr√©requis</h2>
<table>
<thead>
<tr>
<th>Composant</th>
<th>Version Minimale</th>
<th>Recommandation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>3.8+</td>
<td>3.10 √† 3.12</td>
</tr>
<tr>
<td><strong>RAM</strong></td>
<td>8 GB</td>
<td>16 GB ou plus</td>
</tr>
<tr>
<td><strong>GPU</strong></td>
<td>Optionnel</td>
<td>NVIDIA avec 6GB+ VRAM</td>
</tr>
<tr>
<td><strong>Espace Disque</strong></td>
<td>10 GB</td>
<td>20 GB (pour plusieurs mod√®les)</td>
</tr>
</tbody>
</table>
</div>
<div class="card">
<h2>üöÄ Installation Pas √† Pas</h2>
<div class="step">√âtape 1 : Installer Ollama</div>
<h3>Sur Windows</h3>
<pre># T√©l√©charger et installer depuis le site officiel
https://ollama.com/download/windows

# Ou via winget
winget install Ollama.Ollama</pre>
<h3>Sur macOS</h3>
<pre># T√©l√©charger et installer depuis le site officiel
https://ollama.com/download/mac

# Ou via Homebrew
brew install ollama</pre>
<h3>Sur Linux</h3>
<pre># Installation automatique
curl -fsSL https://ollama.com/install.sh | sh

# D√©marrer le service
sudo systemctl start ollama
sudo systemctl enable ollama</pre>
<div class="success-box">
<strong>‚úÖ V√©rification</strong> : Ouvrir un terminal et taper :
                <pre>ollama --version</pre>
                Vous devriez voir la version d'Ollama s'afficher.
            </div>
<div class="step">√âtape 2 : T√©l√©charger un Mod√®le IA</div>
<p>Choisissez un mod√®le selon votre mat√©riel (exemples) :</p>
<table>
<thead>
<tr>
<th>Mod√®le</th>
<th>Taille</th>
<th>RAM Requise</th>
<th>Function Calling</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>qwen2.5:3b</code></td>
<td>2 GB</td>
<td>4 GB</td>
<td>‚úÖ Oui</td>
</tr>
<tr>
<td><code>qwen2.5:7b</code></td>
<td>4.7 GB</td>
<td>8 GB</td>
<td>‚úÖ Oui (Recommand√©)</td>
</tr>
<tr>
<td><code>qwen2.5:14b</code></td>
<td>9 GB</td>
<td>16 GB</td>
<td>‚úÖ Excellent</td>
</tr>
<tr>
<td><code>llama3.2:3b</code></td>
<td>2 GB</td>
<td>4 GB</td>
<td>‚úÖ Oui</td>
</tr>
<tr>
<td><code>mistral:7b</code></td>
<td>4.1 GB</td>
<td>8 GB</td>
<td>‚úÖ Oui</td>
</tr>
<tr>
<td><code>granite3.1-dense:8b</code></td>
<td>4.9 GB</td>
<td>8 GB</td>
<td>‚úÖ Excellent</td>
</tr>
</tbody>
</table>
<h3>T√©l√©charger le mod√®le (exemple : qwen2.5:7b)</h3>
<pre># Dans un terminal
ollama pull qwen2.5:7b

# Attendre le t√©l√©chargement (peut prendre plusieurs minutes)
# Tester le mod√®le
ollama run qwen2.5:7b "Bonjour, peux-tu te pr√©senter ?"</pre>
<div class="warning-box">
<strong>‚ö†Ô∏è Important</strong> : Les mod√®les de moins de 3B de param√®tres sont moins performants pour le function calling. Privil√©giez des mod√®les performants si votre machine le permet.
            </div>
<div class="step">√âtape 3 : Installer le Serveur MCP ComfyUI</div>
<pre># Cloner le projet
git clone https://github.com/orion4d/ComfyUI_mcp.git
cd ComfyUI_mcp

# Cr√©er l'environnement virtuel
python -m venv venv

# Activer l'environnement
# Windows :
venv\Scripts\activate
# macOS/Linux :
source venv/bin/activate

# Installer les d√©pendances
pip install -r requirements.txt

# G√©n√©rer les cl√©s de s√©curit√©
python generate_key.py</pre>
<div class="step">√âtape 4 : Installer l'Interface Gradio Ollama</div>
<pre># Aller dans le dossier de l'interface
cd clients/gradio_ollama

# Installer les d√©pendances sp√©cifiques
pip install -r requirements.txt</pre>
<div class="success-box">
<strong>‚úÖ V√©rification</strong> : Les packages suivants doivent √™tre install√©s :
                <ul>
<li>gradio</li>
<li>openai (pour l'API Ollama compatible)</li>
<li>requests</li>
</ul>
</div>
</div>
<div class="card">
<h2>‚ñ∂Ô∏è D√©marrage</h2>
<div class="step">√âtape 1 : D√©marrer le Serveur MCP</div>
<pre># Dans un terminal (√† la racine du projet)
cd ComfyUI_mcp
python server.py

# Vous devriez voir :
# INFO:     Uvicorn running on http://127.0.0.1:8000</pre>
<div class="step">√âtape 2 : V√©rifier qu'Ollama est Actif</div>
<pre># Dans un autre terminal
ollama serve

# Si Ollama est d√©j√† actif, vous verrez :
# Error: listen tcp 127.0.0.1:11434: bind: address already in use
# (C'est normal et bon signe !)</pre>
<div class="step">√âtape 3 : Lancer l'Interface Gradio</div>
<pre># Dans un troisi√®me terminal
cd ComfyUI_mcp/clients/gradio_ollama
python app.py

# L'interface d√©marre sur :
# Running on local URL:  http://127.0.0.1:7860</pre>
<div class="success-box">
<h3><span class="icon">‚úÖ</span>Pr√™t √† Utiliser !</h3>
<p>Ouvrez votre navigateur et allez sur : <strong>http://127.0.0.1:7860</strong></p>
<p>Vous devriez voir l'interface de chat avec la liste des outils MCP disponibles sur le c√¥t√©.</p>
</div>
</div>
<div class="card">
<h2>üí¨ Utilisation</h2>
<h3>Exemples de Questions</h3>
<div class="info-box">
<ul>
<li>"Liste tous mes workflows ComfyUI disponibles"</li>
<li>"Peux-tu m'aider √† cr√©er un nouveau custom node ?"</li>
<li>"Affiche l'historique de mes g√©n√©rations r√©centes"</li>
<li>"Queue un prompt avec le workflow 'default'"</li>
<li>"Lis le contenu du custom node 'mon_node.py'"</li>
</ul>
</div>
<h3>Activer/D√©sactiver les Outils</h3>
<p>Dans le panneau de droite, vous pouvez :</p>
<ul>
<li>‚úÖ <strong>Activer les outils MCP</strong> : Le mod√®le peut utiliser les outils ComfyUI</li>
<li>‚ùå <strong>D√©sactiver les outils MCP</strong> : Le mod√®le r√©pond uniquement en conversation normale</li>
</ul>
<h3>Changer de Mod√®le</h3>
<p>Pour utiliser un autre mod√®le Ollama, √©ditez le fichier <code>app.py</code> :</p>
<pre># Ligne 11 dans app.py
OLLAMA_MODEL = "qwen2.5:7b"  # Remplacez par votre mod√®le

# Exemples :
OLLAMA_MODEL = "qwen2.5:14b"
OLLAMA_MODEL = "llama3.2:3b"
OLLAMA_MODEL = "mistral:7b"</pre>
</div>
<div class="card">
<h2>üîß Configuration Avanc√©e</h2>
<h3>Modifier le Port de Gradio</h3>
<pre># Dans app.py, √† la fin du fichier :
demo.launch(
    server_name="127.0.0.1",
    server_port=7860,  # Changez le port ici
    share=False
)</pre>
<h3>Modifier l'URL du Serveur MCP</h3>
<pre># Ligne 10 dans app.py
MCP_SERVER_URL = "http://127.0.0.1:8000"

# Si votre serveur MCP est sur un autre port :
MCP_SERVER_URL = "http://127.0.0.1:8080"</pre>
<h3>Activer le Partage Public (Gradio Share)</h3>
<div class="warning-box">
<strong>‚ö†Ô∏è Attention</strong> : Cela expose votre interface sur Internet via un lien temporaire Gradio.
                <pre># Dans app.py
demo.launch(
    server_name="127.0.0.1",
    server_port=7860,
    share=True  # ‚ö†Ô∏è G√©n√®re une URL publique temporaire
)</pre>
<p><strong>Ne faites ceci que si vous comprenez les implications de s√©curit√© !</strong></p>
</div>
<h3>Utiliser un GPU NVIDIA avec Ollama</h3>
<pre># Ollama d√©tecte automatiquement votre GPU NVIDIA
# V√©rifier que CUDA est utilis√© :
ollama run qwen2.5:7b

# Dans les logs, vous devriez voir :
# [CUDA] GPU detected</pre>
</div>
<div class="card">
<h2>‚ùì D√©pannage</h2>
<h3>‚ùå Erreur : "Connection refused" (port 11434)</h3>
<div class="warning-box">
<p><strong>Probl√®me</strong> : Ollama n'est pas d√©marr√©</p>
<p><strong>Solution</strong> :</p>
<pre># D√©marrer Ollama
ollama serve

# Ou sur Linux :
sudo systemctl start ollama</pre>
</div>
<h3>‚ùå Erreur : "No tools available"</h3>
<div class="warning-box">
<p><strong>Probl√®me</strong> : Le serveur MCP n'est pas accessible</p>
<p><strong>Solution</strong> :</p>
<ol>
<li>V√©rifier que le serveur MCP est d√©marr√© (<code>python server.py</code>)</li>
<li>Tester manuellement : <code>curl http://127.0.0.1:8000/debug/health</code></li>
<li>Cliquer sur "üîÑ Rafra√Æchir les outils" dans l'interface</li>
</ol>
</div>
<h3>‚ùå Le mod√®le ne r√©pond pas / tr√®s lent</h3>
<div class="warning-box">
<p><strong>Probl√®me</strong> : Mod√®le trop lourd pour votre machine</p>
<p><strong>Solution</strong> : Essayez un mod√®le plus l√©ger</p>
<pre># Supprimer le mod√®le lourd
ollama rm qwen2.5:14b

# T√©l√©charger un mod√®le plus l√©ger
ollama pull qwen2.5:3b

# Modifier app.py
OLLAMA_MODEL = "qwen2.5:3b"</pre>
</div>
<h3>‚ùå Les outils ne sont pas appel√©s</h3>
<div class="warning-box">
<p><strong>Probl√®me</strong> : Le mod√®le ne supporte pas bien le function calling</p>
<p><strong>Solution</strong> :</p>
<ul>
<li>Utilisez un mod√®le recommand√© : <code>qwen2.5:7b</code>, <code>qwen2.5:14b</code>, ou <code>granite3.1-dense:8b</code></li>
<li>Soyez plus explicite dans vos questions : "Utilise l'outil list_workflows pour lister mes workflows"</li>
</ul>
</div>
</div>



<footer style="text-align: center; margin: 40px 0 20px 0; color: #666;">
<p>ComfyUI MCP Server ¬© 2025</p>
</footer>
</div>
</body>
</html>
